# safe-agent configuration
# Place this file at ~/.config/safe-agent/config.toml

# Agent display name
# agent_name = "safe-agent"

# Core personality (instructions that persist across all interactions)
# core_personality = "You are a helpful autonomous AI assistant."

# Dashboard bind address
# dashboard_bind = "127.0.0.1:3030"

# Agent tick interval in seconds (how often the agent runs maintenance)
# tick_interval_secs = 120

# Number of recent conversation messages to include in context
# conversation_window = 50

# Seconds before unapproved actions expire
# approval_expiry_secs = 3600

[llm]
# Backend: "claude" (Claude Code CLI), "codex" (OpenAI Codex CLI),
#          or "local" (GGUF via llama-gguf, requires --features local)
# Override with LLM_BACKEND env var.
# backend = "claude"

# -- Claude CLI settings (backend = "claude") --

# Path to the `claude` binary (default: "claude")
# Override with CLAUDE_BIN env var
# claude_bin = "claude"

# Claude Code config directory for profile selection
# Override with CLAUDE_CONFIG_DIR env var
# claude_config_dir = ""

# Model to use: "sonnet", "opus", "haiku"
# Override with CLAUDE_MODEL env var
# model = "sonnet"

# Max tool-use turns per invocation
# max_turns = 10

# -- Codex CLI settings (backend = "codex") --

# Path to the `codex` binary (default: "codex")
# Override with CODEX_BIN env var
# codex_bin = "codex"

# Codex model override.  Override with CODEX_MODEL env var.
# codex_model = "gpt-5-codex"

# Codex config profile (from ~/.codex/config.toml).
# Override with CODEX_PROFILE env var.
# codex_profile = ""

# -- Local model settings (backend = "local") --

# Path to the GGUF model file.  Override with MODEL_PATH env var.
# model_path = "/path/to/model.gguf"

# Sampling parameters
# temperature = 0.7
# top_k = 40
# top_p = 0.95
# repeat_penalty = 1.1

# Maximum tokens to generate per response
# max_tokens = 2048

# Maximum context length (0 = use model default).  Lower values reduce VRAM.
# context_length = 4096

# Use GPU acceleration (requires --features local-cuda at compile time)
# use_gpu = false

[tools.exec]
# Enable shell command execution tool
# enabled = true

# Allowed commands (empty = all commands allowed, subject to approval)
# allowed_commands = []

# Command timeout in seconds
# timeout_secs = 30

# Security mode: "approval" (all commands need approval) or "allowlist" (only allowed_commands)
# security = "approval"

[tools.web]
# Enable web search and fetch tools
# enabled = true

# Enable DuckDuckGo safe search
# safe_search = true

# Maximum number of search results
# max_results = 10

# Allowed domains for web_fetch (empty = all domains allowed)
# allowed_domains = []

[tools.browser]
# Enable headless browser automation tool
# enabled = false

# Run browser in headless mode
# headless = true

[tools.message]
# Enable messaging platform tools (Discord, Telegram, Slack, etc.)
# enabled = false

[tools.cron]
# Enable cron scheduling tool
# enabled = false

# Maximum number of concurrent cron jobs
# max_jobs = 50

[telegram]
# Enable Telegram bot interface
# Token must be set via environment variable: TELEGRAM_BOT_TOKEN
# enabled = false

# Only these chat IDs can control the bot (empty = deny all)
# allowed_chat_ids = []

[google]
# Enable Google OAuth2 integration
# Client ID and secret must be set via environment variables:
#   GOOGLE_CLIENT_ID, GOOGLE_CLIENT_SECRET
# enabled = false

# OAuth2 redirect URI (must match Google Cloud Console config)
# redirect_uri = "http://localhost:3030/auth/google/callback"

[sessions]
# Enable multi-agent session coordination
# enabled = false

# Maximum number of concurrent agent sessions
# max_agents = 10
