# safe-agent configuration
# Place this file at ~/.config/safe-agent/config.toml

# Agent display name
# agent_name = "safe-agent"

# Core personality (instructions that persist across all interactions)
# core_personality = "You are a helpful autonomous AI assistant."

# Default timezone (IANA name).  Per-user overrides take precedence.
# The agent includes current local time in its system prompt for time-aware responses.
# timezone = "UTC"

# Default locale for date/number formatting (BCP 47 tag).  Per-user overrides take precedence.
# locale = "en-US"

# Dashboard bind address
# dashboard_bind = "127.0.0.1:3030"

# Agent tick interval in seconds (how often the agent runs maintenance)
# tick_interval_secs = 120

# Number of recent conversation messages to include in context
# conversation_window = 50

# Seconds before unapproved actions expire
# approval_expiry_secs = 3600

# Maximum tool-call round-trips per user message (prevents infinite loops)
# max_tool_turns = 5

# Tools that are auto-approved (no human approval needed)
# auto_approve_tools = ["message", "memory_search", "memory_get"]

[llm]
# Backend: "claude" (Claude Code CLI), "codex" (OpenAI Codex CLI),
#          "gemini" (Google Gemini CLI), "aider" (Aider multi-provider),
#          "openrouter" (OpenRouter API - hundreds of models, one key),
#          or "local" (GGUF via llama-gguf, requires --features local)
# Override with LLM_BACKEND env var.
# backend = "claude"

# -- Claude CLI settings (backend = "claude") --

# Path to the `claude` binary (default: "claude")
# Override with CLAUDE_BIN env var
# claude_bin = "claude"

# Claude Code config directory for profile selection
# Override with CLAUDE_CONFIG_DIR env var
# claude_config_dir = ""

# Model to use: "sonnet", "opus", "haiku"
# Override with CLAUDE_MODEL env var
# model = "sonnet"

# Max tool-use turns per invocation
# max_turns = 10

# -- Codex CLI settings (backend = "codex") --

# Path to the `codex` binary (default: "codex")
# Override with CODEX_BIN env var
# codex_bin = "codex"

# Codex model override.  Override with CODEX_MODEL env var.
# codex_model = "gpt-5-codex"

# Codex config profile (from ~/.codex/config.toml).
# Override with CODEX_PROFILE env var.
# codex_profile = ""

# -- Gemini CLI settings (backend = "gemini") --

# Path to the `gemini` binary (default: "gemini")
# Override with GEMINI_BIN env var
# gemini_bin = "gemini"

# Gemini model override (e.g. "gemini-2.5-pro", "gemini-2.5-flash").
# Override with GEMINI_MODEL env var.
# gemini_model = ""

# -- Aider settings (backend = "aider") --

# Path to the `aider` binary (default: "aider")
# Override with AIDER_BIN env var
# aider_bin = "aider"

# Model string for aider (e.g. "gpt-4o", "claude-3.5-sonnet", "gemini/gemini-2.5-pro").
# Aider supports many providers; set the corresponding API key env var.
# Override with AIDER_MODEL env var.
# aider_model = ""

# -- OpenRouter settings (backend = "openrouter") --
# OpenRouter provides access to hundreds of models (Claude, GPT, Gemini,
# Llama, Mistral, DeepSeek, etc.) through one API key and one endpoint.
# Sign up at https://openrouter.ai/ to get a key.

# API key.  Prefer OPENROUTER_API_KEY env var instead of storing in config.
# openrouter_api_key = ""

# Model identifier.  Browse models at https://openrouter.ai/models
# Examples: "anthropic/claude-sonnet-4", "openai/gpt-4o",
#           "google/gemini-2.5-pro", "meta-llama/llama-4-maverick",
#           "deepseek/deepseek-r1", "mistralai/mistral-large"
# Override with OPENROUTER_MODEL env var.
# openrouter_model = "anthropic/claude-sonnet-4"

# Base URL (default: "https://openrouter.ai/api/v1").
# Change for self-hosted OpenRouter-compatible endpoints.
# Override with OPENROUTER_BASE_URL env var.
# openrouter_base_url = ""

# Max tokens for completions (0 = use general max_tokens setting above)
# openrouter_max_tokens = 0

# Optional: your site URL (sent as HTTP-Referer for OpenRouter analytics)
# Override with OPENROUTER_SITE_URL env var.
# openrouter_site_url = ""

# Optional: your app name (shown in OpenRouter dashboard)
# Override with OPENROUTER_APP_NAME env var.
# openrouter_app_name = "safe-agent"

# -- Local model settings (backend = "local") --

# Path to the GGUF model file.  Override with MODEL_PATH env var.
# model_path = "/path/to/model.gguf"

# Sampling parameters
# temperature = 0.7
# top_k = 40
# top_p = 0.95
# repeat_penalty = 1.1

# Maximum tokens to generate per response
# max_tokens = 2048

# Maximum context length (0 = use model default).  Lower values reduce VRAM.
# context_length = 4096

# Use GPU acceleration (requires --features local-cuda at compile time)
# use_gpu = false

[tools.exec]
# Enable shell command execution tool
# enabled = true

# Allowed commands (empty = all commands allowed, subject to approval)
# allowed_commands = []

# Command timeout in seconds
# timeout_secs = 30

# Security mode: "approval" (all commands need approval) or "allowlist" (only allowed_commands)
# security = "approval"

[tools.web]
# Enable web search and fetch tools
# enabled = true

# Enable DuckDuckGo safe search
# safe_search = true

# Maximum number of search results
# max_results = 10

# Allowed domains for web_fetch (empty = all domains allowed)
# allowed_domains = []

[tools.browser]
# Enable headless browser automation tool
# enabled = false

# Run browser in headless mode
# headless = true

[tools.message]
# Enable messaging platform tools (Discord, Telegram, Slack, etc.)
# enabled = false

[tools.cron]
# Enable cron scheduling tool
# enabled = false

# Maximum number of concurrent cron jobs
# max_jobs = 50

[tls]
# Automatic HTTPS via Let's Encrypt (ACME TLS-ALPN-01 challenge).
# The container will abort if enabled and the certificate cannot be obtained.
# acme_enabled = false

# Domain(s) the certificate covers.  Override with ACME_DOMAIN env var.
# acme_domains = ["agent.example.com"]

# Contact email for Let's Encrypt.  Override with ACME_EMAIL env var.
# acme_email = "admin@example.com"

# Use Let's Encrypt production CA (true) or staging (false, default).
# Start with staging to avoid rate limits while testing.
# acme_production = false

# Directory to cache account keys and certificates.
# Defaults to $XDG_DATA_HOME/safe-agent/acme-cache.
# acme_cache_dir = ""

# HTTPS listen port (default: 443).  Override with ACME_PORT env var.
# acme_port = 443

[tunnel]
# Enable ngrok tunnel for exposing the dashboard (and OAuth callbacks) publicly.
# Also auto-enables if NGROK_AUTHTOKEN is set in the environment.
# enabled = false

# Path to ngrok binary.  Override with NGROK_BIN env var.
# ngrok_bin = "ngrok"

# Auth token.  Prefer NGROK_AUTHTOKEN env var instead of storing in config.
# authtoken = ""

# Static domain from your ngrok dashboard (free tier gets one).
# Override with NGROK_DOMAIN env var.
# domain = ""

# Local port for ngrok's inspection API (leave at 4040 unless you changed it)
# inspect_port = 4040

# How often (seconds) to poll the ngrok API for the tunnel URL
# poll_interval_secs = 15

[dashboard]
# Whether password-based login is enabled (default: true).
# Set to false to require SSO-only login (requires sso_providers to be set).
# password_enabled = true

# SSO providers enabled for dashboard login.
# Uses the same OAuth client credentials as the Connected Accounts system.
# Provider IDs: "google", "github", "microsoft", "discord", "linkedin", etc.
# sso_providers = ["google", "github"]

# Email addresses allowed to sign in via SSO.
# Empty = any authenticated SSO user is allowed.
# sso_allowed_emails = ["admin@example.com"]

[telegram]
# Enable Telegram bot interface
# Token must be set via environment variable: TELEGRAM_BOT_TOKEN
# enabled = false

# Only these chat IDs can control the bot (empty = deny all)
# allowed_chat_ids = []

[whatsapp]
# Enable WhatsApp bot interface via Baileys (Node.js bridge)
# enabled = false

# Port for the Baileys bridge HTTP API
# bridge_port = 3033

# Dashboard port (used to construct webhook URL for incoming messages)
# webhook_port = 3030

# Allowed phone numbers (E.164 format, empty = deny all)
# allowed_numbers = ["+1234567890"]

# Note: Google API integration (Calendar, Drive, Docs) is handled through the
# skill system.  Skills declare [[credentials]] in their skill.toml for any
# API keys or OAuth secrets they need; configure values via the dashboard UI.

[security]
# Tools that are completely blocked (never executable, regardless of approval)
# blocked_tools = []

# Tools that require 2FA (second-channel confirmation) before execution
# Default: ["exec"] — shell commands require dashboard confirmation
# require_2fa = ["exec"]

# Maximum tool calls per minute (0 = unlimited)
# rate_limit_per_minute = 30

# Maximum tool calls per hour (0 = unlimited)
# rate_limit_per_hour = 300

# Maximum estimated LLM cost per day in USD (0.0 = unlimited)
# daily_cost_limit_usd = 0.0

# Enable PII/sensitive data detection in LLM responses
# Flags SSNs, credit cards, API keys, passwords, etc.
# pii_detection = true

# Fine-grained capability restrictions per tool.
# Keys are tool names, values are lists of allowed operations.
# If a tool is listed here, ONLY the specified operations are permitted.
# Examples:
#   exec = ["ls", "cat", "echo", "grep"]    # only these commands allowed
#   file = ["read"]                           # read-only file access
# [security.tool_capabilities]
# exec = ["ls", "cat", "echo", "grep", "find", "wc"]

# ── Federation ──────────────────────────────────────────────────
# Multi-node federation allows multiple safe-agent instances to share
# memory and coordinate tasks.
[federation]

# Enable federation (default: false)
# enabled = false

# Display name for this node (defaults to agent_name)
# node_name = "node-1"

# The address peers use to connect back to this node.
# Must be reachable from all peer nodes.
# advertise_address = "http://192.168.1.100:3031"

# Peer addresses to connect to on startup.
# peers = ["http://192.168.1.101:3031", "http://192.168.1.102:3031"]

# Heartbeat interval in seconds (default: 30)
# heartbeat_interval_secs = 30

# Memory sync interval in seconds (default: 5)
# sync_interval_secs = 5

[memory]
# Ollama model used for generating embeddings (semantic search over memories).
# Set to empty string to disable embeddings and fall back to FTS5.
# embedding_model = "nomic-embed-text"

# Ollama host for embedding requests (defaults to llm.ollama_host, then OLLAMA_HOST).
# embedding_host = ""

# Automatically extract facts, preferences, and entities after each conversation.
# auto_extract = true

# Consolidate archival memories older than this many days (reduces context bloat).
# consolidation_age_days = 30

# Maximum old memories to consolidate per tick.
# consolidation_batch_size = 20

[sessions]
# Enable multi-agent session coordination
# enabled = false

# Maximum number of concurrent agent sessions
# max_agents = 10
